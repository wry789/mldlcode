{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_data(file):\n",
    "    \"\"\"数据读取函数\"\"\"\n",
    "    with open(file,'rb') as f:\n",
    "        sent_lags=[] #用于临时存放一个中文句子和存放一个句子对应的标注\n",
    "        # 每次读取一行数据\n",
    "        for line in f:\n",
    "            line = line.decode('utf8')\n",
    "            sentence = line.split('。') # 每一行为一个段落，按句号将段落切分成句子\n",
    "            for sent in sentence: # 每一个句子\n",
    "                sent = sent.strip()\n",
    "                if not sent:\n",
    "                    continue \n",
    "                wordslist = sent.split(\"  \")  # 将句子按空格进行切分，得到词\n",
    "                _sent_lags = []\n",
    "                for word in wordslist: # word:\"我爱你\"\n",
    "                    sentlist = list(word)# ['我', '爱', '你'] \n",
    "                    tagslist = get_tag(word)# ['B', 'M', 'E']\n",
    "                    _sent_lags.extend(list(zip(sentlist,tagslist))) # [('我', 'B'), ('爱', 'M'), ('你', 'E')]\n",
    "                sent_lags.append(_sent_lags)   \n",
    "    return sent_lags\n",
    "\n",
    "def get_tag(word):\n",
    "    \"\"\"\n",
    "    将词转化为标签的函数\n",
    "    get_tag(\"我爱你\") >> ['B', 'M', 'E']\n",
    "    \n",
    "    \"\"\"\n",
    "    tags = []           #创建一个空列表用来存放标注数据\n",
    "    word_len = len(word)\n",
    "    if word_len == 1:   #如果是单字成词，标记为 S\n",
    "        tags = ['S']\n",
    "    elif word_len == 2: # 如果该词仅有两个字，则标记为 B 和 E\n",
    "        tags = ['B', 'E']\n",
    "    else:\n",
    "        tags.append('B')     #第一个字标记为 B\n",
    "        tags.extend(['M']*(len(word)-2)) #中间标记为 M ，\n",
    "        tags.append('E')     #最后一个标记为 E\n",
    "    return tags\n",
    "\n",
    "# def pre_data(data,ifsent=True):\n",
    "#     \"\"\"数据预处理函数，得到句子和标签\"\"\"\n",
    "#     sent_lags=[] #用于临时存放一个中文句子,时存放一个句子对应的标注\n",
    "#     for sentence in data:\n",
    "#         sentence = sentence.strip()\n",
    "#         if not sentence:\n",
    "#             continue\n",
    "        \n",
    "#         words = sentence.split(\"  \")\n",
    "#         _sent_lags = []\n",
    "#         for word in words:\n",
    "#             sent = list(word)\n",
    "#             tags = get_tag(word)#获得标注结果\n",
    "#             _sent_lags.extend(list(zip(sent,tags)))\n",
    "#         sent_lags.append(_sent_lags)\n",
    "#     return sent_lags\n",
    "\n",
    "def word2features(sent,i):\n",
    "    \"\"\"返回特征列表\"\"\"\n",
    "    word = sent[i][0] #句子的目标字\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1 word': word1,\n",
    "            '-1:0 word': word1 + word,\n",
    "            '-1 word.isdigit()':word1.isdigit(),\n",
    "            \n",
    "            })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "     \n",
    "    if i > 1:\n",
    "        word2 = sent[i-2][0]\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-2 word': word2,\n",
    "            '-2:0 word': word2+word1+word,\n",
    "            '-2 word.isdigit()': word2.isdigit(),\n",
    "        })    \n",
    "        \n",
    "    if i > 2:\n",
    "        word3 = sent[i - 3][0]\n",
    "        word2 = sent[i - 2][0]\n",
    "        word1 = sent[i - 1][0]\n",
    "        features.update({\n",
    "            '-3 word': word3,\n",
    "            '-3:0 word ': word3+word2+word1+word,\n",
    "            '-3 word.isdigit()': word3.isdigit(),\n",
    "        })        \n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '1 word': word1,\n",
    "            '0:1 word': word+word1,\n",
    "            '1 word.isdigit()': word1.isdigit(),\n",
    "        })        \n",
    "        \n",
    "    else:\n",
    "        features['EOS'] = True        \n",
    "\n",
    "    if i < len(sent)-2:\n",
    "        word2 = sent[i + 2][0]\n",
    "        word1 = sent[i + 1][0]\n",
    "        features.update({\n",
    "            '2 word': word2,\n",
    "            '0:2 word': word + word1 + word2,\n",
    "            '2 word.isdigit()': word2.isdigit(),\n",
    "        })        \n",
    " \n",
    "    if i < len(sent)-3:\n",
    "        word3 = sent[i + 3][0]\n",
    "        word2 = sent[i + 2][0]\n",
    "        word1 = sent[i + 1][0]\n",
    "        features.update({\n",
    "            '3 word': word3,\n",
    "            '0:3 words':word + word1 + word2 + word3,\n",
    "            '3 word.isdigit()': word3.isdigit(),\n",
    "        })\n",
    "        \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [ele[-1] for ele in sent]\n",
    "\n",
    "def split_train_test(data_X,data_y,p=0.7):\n",
    "    \"\"\"划分训练集和测试集函数\"\"\"\n",
    "    spl=int(len(data_X)*p)\n",
    "    train_X=data_X[:spl]\n",
    "    test_X=data_X[spl:]\n",
    "    train_y=data_y[:spl]\n",
    "    test_y=data_y[spl:]\n",
    "    return train_X,train_y,test_X,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain_sent_lags=load_pre_data(\"E:\\\\corpus\\\\icwb2-data\\\\training\\\\pku_training.utf8\")\n",
    "X_train = [sent2features(s) for s in datatrain_sent_lags]\n",
    "y_train = [sent2labels(s) for s in datatrain_sent_lags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatest_sent_lags = load_pre_data(\"E:\\\\corpus\\\\icwb2-data\\\\gold\\\\pku_test_gold.utf8\")\n",
    "X_test = [sent2features(s) for s in datatest_sent_lags]\n",
    "y_test = [sent2labels(s) for s in datatest_sent_lags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crf_model = sklearn_crfsuite.CRF(algorithm='l2sgd',max_iterations=100,c2=0.1,\n",
    "                                 all_possible_transitions=True,verbose=True)\n",
    "crf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf_model.predict(X_test)\n",
    "labels = list(crf_model.classes_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B      0.955     0.954     0.955     56883\n",
      "           E      0.952     0.951     0.952     56883\n",
      "           M      0.740     0.891     0.808     11480\n",
      "           S      0.953     0.904     0.928     44063\n",
      "\n",
      "    accuracy                          0.936    169309\n",
      "   macro avg      0.900     0.925     0.911    169309\n",
      "weighted avg      0.939     0.936     0.937    169309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\corpus\\\\crf_model.pkl']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(crf_model, \"E:\\\\corpus\\\\crf_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "new_sents = []\n",
    "text = \"人要自强不息，每天都要有进步。\"\n",
    "new_sents.append(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_tagger = joblib.load('E:\\\\corpus\\\\crf_model.pkl')\n",
    "sents_feature = [sent2features(sent) for sent in new_sents]\n",
    "y_pred = NER_tagger.predict(sents_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_result = []\n",
    "for sent, ner_tag in zip(new_sents, y_pred):\n",
    "    for word, tag in zip(sent, ner_tag):\n",
    "        list_result.append((word,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('人', 'S'),\n",
       " ('要', 'S'),\n",
       " ('自', 'B'),\n",
       " ('强', 'M'),\n",
       " ('不', 'M'),\n",
       " ('息', 'E'),\n",
       " ('，', 'S'),\n",
       " ('每', 'B'),\n",
       " ('天', 'E'),\n",
       " ('都', 'S'),\n",
       " ('要', 'S'),\n",
       " ('有', 'S'),\n",
       " ('进', 'B'),\n",
       " ('步', 'E'),\n",
       " ('。', 'S')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "100"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

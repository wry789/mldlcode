{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"E:\\\\corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:07<00:00, 1662.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:07<00:00, 1778.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:06<00:00, 2018.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:09<00:00, 1361.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def read_imdb(folder='train', data_root=\"E:\\\\corpus\\\\aclImdb\"): \n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "train_data, test_data = read_imdb('train'), read_imdb('test')\n",
    "#train_data\n",
    "# [[\"another reason to watch this delightful movie is florence rice. florence who? that was my first reaction as the opening credits ran on the screen. i soon found out who florence rice was, a real beauty who turns in a simply wonderful performance. as they all do in this gripping ensemble piece. from 1939, its a different time but therein lies the charm. it transports you into another world. it starts out as a light comedy but then turns very serious. florence rice runs the gamut from comedienne to heroine. she is absolutely delightful, at the same time strong, vulnerable evolving from a girl to a woman.watch her facial expressions at the end of the movie. she made over forty movies, and i am going to seek out the other thirty nine. alan marshal is of the flynn/gable mode and proves a perfect match for florence. buddy ebsen and una merkel provide some excellent comic moments, but the real star is florence rice. fans of 30's/40's movies, don't miss this one!\",\n",
    "#   1],\n",
    "#  [\"i gave timecop a perfect 10, i gave this 1<br /><br />it's story is very boring, and it has only little to do with the original timecop. lots of things from timecop was scrapped, and they put in new stupid stuff instead. this story is taking place in 2060 (if i remember correctly), but for some reason the timetraveling is now more dangerous :confused:<br /><br />and the action scenes are nothing to be happy about, well most of them aren't... only the first one is great... and there aren't many action scenes at all, and they're all pretty short<br /><br />at one point in the story, the main character travels through time about 5 times within a few minutes... no wait, make that two times...<br /><br />in short: don't waste time watching this movie, it's not worth it\",\n",
    "#   0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_imdb(data):\n",
    "    \"\"\"\n",
    "    data: list of [string, label]\n",
    "    \"\"\"\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    return [tokenizer(review) for review, _ in data]\n",
    "\n",
    "# return \n",
    "# [['another',\n",
    "#   'reason',\n",
    "#     ...\n",
    "#     'miss',\n",
    "#   'this',\n",
    "#   'one!'],\n",
    "#  ['i',\n",
    "#   'gave',\n",
    "#   'timecop'\n",
    "#   ...\n",
    "#   'not',\n",
    "#   'worth',\n",
    "#   'it']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('# words in vocab:', 46152)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab_imdb(data):\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "#     len([st for st in get_tokenized_imdb(train_data)])=25000  25,000条评论\n",
    "#     [['i','cannot','believe',..., 'taste','in','movies.']...]\n",
    "# len([tk for st in tokenized_data for tk in st])=5844418  25000条评论的所有字数\n",
    "# counter：Counter({'i': 70477,'cannot': 1089,'believe': 2309,'the': 322174,'same': 3770,...,'ending.<br': 55,'worst': 2440,...})\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5) # 默认添加特殊词[‘<unk’>, ‘<pad>’]到词汇表中\n",
    "\n",
    "# vocab vocabulary object\n",
    "# Vocab.freqs:各个词的频数\n",
    "# Vocab.stoi：各个词的索引\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "'# words in vocab:', len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb(data, vocab):\n",
    "    max_l = 500  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "#     words : ['i','cannot','believe','the',...'taste','in','movies.']    \n",
    "#     [vocab.stoi[word] for word in words]: [9,486,250,2,...,1743,8,560]\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data]) # 长度变为500，值为词索引标识符\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels\n",
    "# return tuple\n",
    "# (tensor([[ 167,  307,    6,  ...,    0,    0,    0],\n",
    "#          [   9,  441,    0,  ...,    0,    0,    0],\n",
    "#          [2068,    2,   58,  ...,    0,    0,    0],\n",
    "#          ...,\n",
    "#          [   9,  585,    9,  ...,    0,    0,    0],\n",
    "#          [  46,    9,   90,  ...,    0,    0,    0],\n",
    "#          [   9,   98,   10,  ...,    0,    0,    0]]),\n",
    "#  tensor([1, 0, 0,  ..., 0, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n",
    "# train_set:25000行，500列，25000个评论\n",
    "# (tensor([[ 167,  307,    6,  ...,    0,    0,    0], \n",
    "#          [   9,  441,    0,  ...,    0,    0,    0],\n",
    "#          [2068,    2,   58,  ...,    0,    0,    0],\n",
    "#          ...,\n",
    "#          [   9,  585,    9,  ...,    0,    0,    0],\n",
    "#          [  46,    9,   90,  ...,    0,    0,    0],\n",
    "#          [   9,   98,   10,  ...,    0,    0,    0]]),\n",
    "#  tensor([1, 0, 0,  ..., 0, 1, 0]))\n",
    "\n",
    "# train_iter.batch_sampler\n",
    "#     >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
    "#     [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
    "#     >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n",
    "#     [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "\n",
    "# len(train_iter.batch_sampler)\n",
    "# 总共有391个batch\n",
    "# 每个batch有两个tensor，一个为训练数据，大小为torch.Size([64, 500])，一个为标注，大小为torch.Size([64])\n",
    "# [tensor([[ 5039,   274,    44,  ...,     0,     0,     0],\n",
    "#          [  221, 37596,     7,  ...,     0,     0,     0],\n",
    "#          [ 1309,  3580,   175,  ..., 15343,     8,     3],\n",
    "#          ...,\n",
    "#          [   52,     7,   143,  ...,     0,     0,     0],\n",
    "#          [    9,    67,    89,  ...,     0,     0,     0],\n",
    "#          [  415,   210,    70,  ...,     0,     0,     0]]),\n",
    "#  tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
    "#          1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
    "#          0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 500]) y torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches:', 391)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, \n",
    "                                hidden_size=num_hiddens, \n",
    "                                num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        # h,c都有信息，双向则再*2，最后为隐藏单元*4\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是(批量大小, 词数)，因为LSTM需要将序列长度(seq_len)作为第一维，所以将输入转置后\n",
    "        # 再提取词特征，输出形状为(词数, 批量大小, 词向量维度)\n",
    "#         inputs.shape: torch.Size([64, 500])\n",
    "#         print(inputs.permute(1, 0).shape):torch.Size([500, 64]) ,置换tensor中的维度\n",
    "#         print(embeddings.shape): torch.Size([500, 64, 100])\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        \n",
    "        # outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)\n",
    "        outputs, _ = self.encoder(embeddings) # output, (h, c)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为\n",
    "        # (批量大小, 4 * 隐藏单元个数)。\n",
    "#         print(outputs[0].shape) : torch.Size([64, 200])\n",
    "#         print(outputs[-1].shape) : torch.Size([64, 200])\n",
    "#           print(encoding.shape) : torch.Size([64, 400])\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "       \n",
    "        outs = self.decoder(encoding)\n",
    "        return outs\n",
    "    \n",
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)\n",
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=os.path.join(DATA_ROOT, \"glove\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "#     len(words)=46152 # 使用的语料库的词数\n",
    "#     glove_vocab.vectors[0].shape[0]=100 选择的预训练词向量的维度\n",
    "#     embed.shape torch.Size([46152, 100])\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary 未登录词\n",
    "    # glove_vocab.stoi中未登录'<unk>'和'<pad>'\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "#     embed:每个词的100维词向量\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21202 oov words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.2512,  0.6499, -0.2465,  ...,  0.0659, -0.9114,  0.4129],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1752,  0.1468, -0.0800,  ...,  0.1581, -0.6230, -0.2806]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.2409, train acc 0.905, test acc 0.877, time 43.9 sec\n",
      "epoch 2, loss 0.0579, train acc 0.960, test acc 0.849, time 44.1 sec\n",
      "epoch 3, loss 0.0179, train acc 0.982, test acc 0.857, time 44.1 sec\n",
      "epoch 4, loss 0.0083, train acc 0.989, test acc 0.848, time 44.0 sec\n",
      "epoch 5, loss 0.0059, train acc 0.991, test acc 0.856, time 43.9 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "# 要过滤掉不计算梯度的embedding参数\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    \"\"\"sentence是词语的列表\"\"\"\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    return 'positive' if label.item() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'great']) # positive\n",
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'bad']) # negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：http://tangshusen.me/Dive-into-DL-PyTorch/#/chapter10_natural-language-processing/10.7_sentiment-analysis-rnn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
